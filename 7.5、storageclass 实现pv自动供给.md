### 1、什么是StorageClass

Kubernetes集群管理员通过提供不同的存储类，可以满足用户不同的服务质量级别、备份策略和任意策略要求的存储需求。动态存储卷供应使用StorageClass进行实现，其允许存储卷按需被创建。如果没有动态存储供应，Kubernetes集群的管理员将不得不通过手工的方式类创建新的存储卷。通过动态存储卷，Kubernetes将能够按照用户的需要，自动创建其需要的存储。

### 2、 为什么需要StorageClass

在一个大规模的Kubernetes集群里,可能有成千上万个PVC,这就意味着运维人员必须实现创建出这个多个PV,此外,随着项目的需要,会有新的PVC不断被提交,那么运维人员就需要不断的添加新的,满足要求的PV,否则新的Pod就会因为PVC绑定不到PV而导致创建失败.而且通过 PVC 请求到一定的存储空间也很有可能不足以满足应用对于存储设备的各种需求
而且不同的应用程序对于存储性能的要求可能也不尽相同，比如读写速度、并发性能等，为了解决这一问题，Kubernetes 又为我们引入了一个新的资源对象：StorageClass，通过 StorageClass 的定义，管理员可以将存储资源定义为某种类型的资源，比如快速存储、慢速存储等，用户根据 StorageClass 的描述就可以非常直观的知道各种存储资源的具体特性了，这样就可以根据应用的特性去申请合适的存储资源了。

### 3、实现原理：

存储控制器 Volume Controller，是用来专门处理持久化存储的控制器，其一个子控制循环 PersistentVolumeController 负责实现 PV 和 PVC 的绑定。PersistentVolumeController 会 watch kube-apiserver 的 PVC 对象。如果发现有 PVC对象创建，则会查看所有可用的 PV， 如果有则绑定，若没有，则会使用 StorageClass 的配置和 PVC 的描述创建 PV 进行绑定。所谓将一个 PV 与 PVC进行“绑定”，其实就是将这个PV对象的名字，填在了 PVC对象的 spec.volumeName字段上。

![img](https://raw.githubusercontent.com/wzxmt/images/master/img/StorageClass)



### 4、项目地址：

https://github.com/kubernetes-sigs/sig-storage-lib-external-provisioner/releases

### 5、搭建StorageClass+NFS

搭建StorageClass+NFS,大致有以下几个步骤:

```
1.创建一个可用的NFS Serve
2.创建Service Account.这是用来管控NFS provisioner在k8s集群中运行的权限
3.创建StorageClass.负责建立PVC并调用NFS provisioner进行预定的工作,并让PV与PVC建立管理
4.创建NFS provisioner.有两个功能,一个是在NFS共享目录下创建挂载点(volume),另一个则是建了PV并将PV与NFS的挂载点建立关联  
```

### 6、搭建NFS服务

选择存储服务器，搭建nfs

```bash
yum install -y  nfs-utils  rpcbind
cat << EOF >/etc/exports
/data/n1 *(rw,no_root_squash,no_all_squash,sync)
EOF
mkdir -p /data/n{1,2,3}
chmod 666 /data/
chown nfsnobody. /data/
systemctl enable --now rpcbind
systemctl enable --now nfs
```

所有node节点需要安装nfs客户端

```bash
yum install -y  nfs-utils
```

### 7、搭建StorageClass

参考项目：

```
https://github.com/kubernetes-sigs/sig-storage-lib-external-provisioner/releases/tag/v5.0.0
```

创建项目路径，生成相关配置文件

```
mkdir -p nfs-client
cd nfs-client
```

deployment.yaml

```yaml
cat << EOF > deployment.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nfs-client-provisioner
---
kind: Deployment
apiVersion: apps/v1
metadata:
  name: nfs-client-provisioner
spec:
  replicas: 1
  selector:    
    matchLabels:      
      app: nfs-client-provisioner      
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: nfs-client-provisioner
    spec:
      serviceAccountName: nfs-client-provisioner
      containers:
        - name: nfs-client-provisioner
          image: quay.io/external_storage/nfs-client-provisioner:latest
          volumeMounts:
            - name: nfs-client-root
              mountPath: /persistentvolumes
          env:
            - name: PROVISIONER_NAME
              value: fuseim.pri/ifs
            - name: NFS_SERVER
              value: 10.0.0.20
            - name: NFS_PATH
              value: /data/n1
      volumes:
        - name: nfs-client-root
          nfs:
            server: 10.0.0.20
            path: /data/nfs-volume/StorageClass
EOF
```

class.yaml

```yaml
cat<< EOF > class.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: managed-nfs-storage
provisioner: fuseim.pri/ifs # or choose another name, must match deployment's env PROVISIONER_NAME'
parameters:
  archiveOnDelete: "false"
EOF
```

rbac.yaml

```yaml
cat<< EOF > rbac.yaml
kind: ServiceAccount
apiVersion: v1
metadata:
  name: nfs-client-provisioner
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nfs-client-provisioner-runner
rules:
  - apiGroups: [""]
    resources: ["persistentvolumes"]
    verbs: ["get", "list", "watch", "create", "delete"]
  - apiGroups: [""]
    resources: ["persistentvolumeclaims"]
    verbs: ["get", "list", "watch", "update"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["create", "update", "patch"]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: run-nfs-client-provisioner
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    namespace: default
roleRef:
  kind: ClusterRole
  name: nfs-client-provisioner-runner
  apiGroup: rbac.authorization.k8s.io
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-client-provisioner
rules:
  - apiGroups: [""]
    resources: ["endpoints"]
    verbs: ["get", "list", "watch", "create", "update", "patch"]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-client-provisioner
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    # replace with namespace where provisioner is deployed
    namespace: default
roleRef:
  kind: Role
  name: leader-locking-nfs-client-provisioner
  apiGroup: rbac.authorization.k8s.io
EOF
```

部署

```bash
[root@m1 nfs-client]#  kubectl apply -f .
storageclass.storage.k8s.io/managed-nfs-storage created
serviceaccount/nfs-client-provisioner created
deployment.apps/nfs-client-provisioner created
serviceaccount/nfs-client-provisioner created
clusterrole.rbac.authorization.k8s.io/nfs-client-provisioner-runner created
clusterrolebinding.rbac.authorization.k8s.io/run-nfs-client-provisioner created
role.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created
rolebinding.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created
```

查看此NFS插件的pod是否部署成功

```bash
[root@m1 nfs-client]# kubectl get pods
NAME                                      READY   STATUS      RESTARTS   AGE
nfs-client-provisioner-666d85895c-qxmnp   1/1     Running     0          15m
```

查看storageclass

```bash
[root@m1 nfs-client]# kubectl get storageclasses.storage.k8s.io
NAME                  PROVISIONER      RECLAIMPOLICY   VOLUMEBINDINGMODE   
managed-nfs-storage   fuseim.pri/ifs   Delete          Immediate       
```

### 8、测试storageclass  pv自动供给

创建测试pvc

```yaml
cat << EOF >test-claim.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: test-claim
  annotations:
    volume.beta.kubernetes.io/storage-class: "managed-nfs-storage"
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 10Mi
EOF
kubectl apply -f test-claim.yaml
```

创建测试pod

```yaml
cat<< EOF > test-pod.yaml
kind: Pod
apiVersion: v1
metadata:
  name: test-pod
spec:
  containers:
  - name: test-pod
    image: busybox:1.28
    command:
      - "/bin/sh"
    args:
      - "-c"
      - "touch /mnt/SUCCESS && exit 0 || exit 1"
    volumeMounts:
      - name: nfs-pvc
        mountPath: "/mnt"
  restartPolicy: "Never"
  volumes:
    - name: nfs-pvc
      persistentVolumeClaim:
        claimName: test-claim
EOF
kubectl apply -f test-pod.yaml
```

检查是否创建文件

```
[root@n1 ~]# ll /data/n1/default-test-claim-pvc-24ddcb11-4b2c-41a0-b28f-888b282594a8/SUCCESS
-rw-r--r-- 1 root root 0 May  7 22:17 /data/n1/default-test-claim-pvc-24ddcb11-4b2c-41a0-b28f-888b282594a8/SUCCESS
```

可以看到创建的文件，说明storageclass  pv自动供给成功！

### 9、使用StatefulSet创建mongdb集群

为了完成MongoDB集群的搭建，需要创建如下三个资源对象。

- 一个StorageClass，用于StatefulSet自动为各个应用Pod申请PVC。
- 一个Headless Service，用于维护MongoDB集群的状态。
- 一个StatefulSet。

StorageClass

```
cat<< EOF > mongo-StorageClass.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: mongodb-data
provisioner: fuseim.pri/ifs
EOF
kubectl apply -f mongo-StorageClass.yaml
```

Headless Service

```yaml
cat<< EOF > mongo-Headless.yaml
apiVersion: v1
kind: Namespace
metadata:
   name: mongo
   labels:
     name: mongo
---
apiVersion: v1
kind: Service
metadata:
  name: mongo
  namespace: mongo
  labels:
    name: mongo
spec:
  selector:
    role: mongo
  ports:
  - port: 27017
    targetPort: 27017
  clusterIP: None
EOF
kubectl apply -f mongo-Headless.yaml
```

StatefulSet
```
cat<< EOF > mongo-StatefulSet.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mongo
  namespace: mongo
spec:
  serviceName: mongo
  replicas: 3
  selector:    
    matchLabels:      
      role: mongo
      environment: test
  template:
    metadata:
      labels:
        role: mongo
        environment: test
    spec:
      terminationGracePeriodSeconds: 10
      containers:
      - name: mongo
        image: mongo:3.4.22
        command:
        - mongod
        - "--replSet"
        - rs0
        - "--smallfiles"
        - "--noprealloc"
        ports:
        - containerPort: 27017
        volumeMounts:
        - name: mongo-persistent-storage
          mountPath: /data/db
      - name: mongo-sidecar
        image: cvallance/mongo-k8s-sidecar
        env:
        - name: MONGO_SIDECAR_POD_LABELS
          value: "role=mongo,environment=test"
        - name: KUBERNETES_MONGO_SERVICE_NAME
          value: "mongo"
  volumeClaimTemplates:
  - metadata:
      name: mongo-persistent-storage
    spec:
      accessModes: 
      - ReadWriteOnce
      storageClassName: mongodb-data
      resources:
        requests:
          storage: 10Gi
EOF
kubectl apply -f mongo-StatefulSet.yaml
```

查看部署状态

```bash
[root@m1 mongo]# kubectl get pod -n mongo
NAME      READY   STATUS    RESTARTS   AGE
mongo-0   2/2     Running   0          7h55m
mongo-1   2/2     Running   0          7h55m
mongo-2   2/2     Running   0          7h55m
[root@m1 mongo]# kubectl get svc -n mongo
NAME    TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)     AGE
mongo   ClusterIP   None         <none>        27017/TCP   7h54m
[root@m1 mongo]# kubectl get pvc -n mongo
NAME                               STATUS   VOLUME                                   
mongo-persistent-storage-mongo-0   Bound    pvc-d91857e7-4781-4555-b7d8-4af85232d6a2 
mongo-persistent-storage-mongo-1   Bound    pvc-c529d808-33fc-4546-93d4-e14e9d01e553 
mongo-persistent-storage-mongo-2   Bound    pvc-93782f58-fce0-43c3-8e0f-6994b4dd7529 
```

查看mongo集群状态

```bash
[root@m1 mongo]# kubectl exec -it mongo-0 -n mongo -- mongo
> rs.status()
{
        "info" : "run rs.initiate(...) if not yet done for the set",
        "ok" : 0,
        "errmsg" : "no replset config has been received",
        "code" : 94,
        "codeName" : "NotYetInitialized"
}
> 
```

这时候发现集群状态是不可用的。

查看日志

```bash
[root@m1 mongo]# kubectl logs mongo-0 mongo-sidecar -n mongo
···
Error in workloop { [Error: [object Object]]
  message:
   { kind: 'Status',
     apiVersion: 'v1',
     metadata: {},
     status: 'Failure',
     message:
      'pods is forbidden: User "system:serviceaccount:mongo:default" cannot list resource "pods" in API group "" at the cluster scope',
     reason: 'Forbidden',
     details: { kind: 'pods' },
     code: 403 },
  statusCode: 403 }
```

可以看出mongo sidecar没有正确的配置，信息显示默认分配的sa账号没有list此namespace下pods的权限，需要给默认的sa账号提权；

增加list pods的权限

```yaml
cat << EOF >mongo-rabc-list.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: mongo
  name: mongo-pod-read
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "watch", "list"]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: mongo-pod-read
  namespace: mongo
subjects:
- kind: ServiceAccount
  name: default
  namespace: mongo
roleRef:
  kind: Role
  name: mongo-pod-read
  apiGroup: rbac.authorization.k8s.io
  EOF
  kubectl apply -f mongo-rabc-list.yaml
```

使用后仍然无用的配置,这时候sa需要更大的权限，这里使用默认的clusterrole view权限进行赋权，我们可以使用clusterrole对sa进行界定namespace的赋权

```yaml
cat << EOF >mongo-rabc-view.yaml
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: mongo-default-view
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: view
subjects:
  - kind: ServiceAccount
    name: default
    namespace: mongo
  EOF
  kubectl apply -f mongo-rabc-view.yaml
```

重载mongo集群

```
kubectl scale statefulset mongo -n mongo --replicas=0
#重载
kubectl scale statefulset mongo -n mongo --replicas=3
```

重载完毕，查看mongo集群状态

```bash
[root@m1 mongo]# kubectl exec -it mongo-0 -n mongo -- mongo
rs0:SECONDARY> rs.status()
{
        "set" : "rs0",
        "date" : ISODate("2020-05-08T01:27:18.054Z"),
        "myState" : 2,
        "term" : NumberLong(1),
        "syncingTo" : "mongo-1.mongo.mongo.svc.cluster.local:27017",
        "syncSourceHost" : "mongo-1.mongo.mongo.svc.cluster.local:27017",
        "syncSourceId" : 0,
        "heartbeatIntervalMillis" : NumberLong(2000),
......
```

发现状态已经正常,表示集群部署成功。

### 10、 使用/访问

mongo cluster访问默认连接为：

```bash
mongodb://mongo1,mongo2,mongo3:27017/dbname_?
```

在kubernetes中最常用的FQDN连接服务的连接为：

```bash
appName.$HeadlessServiceName.$Namespace.svc.cluster.local
```

因为我们采用statefulset部署的pod，所以命名均有规则，所以实际上如果连接4副本的mongodb cluster，上面的默认连接该为（默认为namespace之外）：

```bash
mongodb://mongo-0.mongo.mongo.svc.cluster.local:27017,mongo-1.mongo.mongo.svc.cluster.local:27017,mongo-2.mongo.mongo.svc.cluster.local:27017,mongo-3.mongo.mongo.svc.cluster.local:27017/?replicaSet=rs0
```