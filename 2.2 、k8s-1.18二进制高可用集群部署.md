## 1. 系统优化

#### 1.1 关闭,关闭selinux与swap

**关闭防火墙**

```bash
firewall-cmd --state
systemctl stop firewalld.service
systemctl disable firewalld.service
```

**关闭selinux**

```bash
setenforce 0
sed -i "s/^SELINUX=enforcing/SELINUX=disabled/g" /etc/selinux/config
```

**关闭swap**

```bash
swapoff -a
echo "vm.swappiness = 0" >>/etc/sysctl.d/kubernetes.conf
sed -i 's/.*swap.*/#&/' /etc/fstab
```

因为这里本次用于测试两台主机上还运行其他服务，关闭swap可能会对其他服务产生影响，所以这里修改kubelet的配置去掉这个限制。 使用kubelet的启动参数--fail-swap-on=false去掉必须关闭swap的限制，修改vim /etc/sysconfig/kubelet，加入：

```bash
cat<< EOF >/etc/sysconfig/kubelet
KUBELET_EXTRA_ARGS=--fail-swap-on=false
EOF
```

#### 1.2 更新 yum 源

```bash
cd /etc/yum.repos.d
mv CentOS-Base.repo CentOS-Base.repo.bak
mv epel.repo  epel.repo.bak
curl https://mirrors.aliyun.com/repo/Centos-7.repo -o CentOS-Base.repo 
curl https://mirrors.aliyun.com/repo/epel-7.repo -o epel.repo
cd -
```

#### 1.3 设置系统参数 

允许路由转发，不对bridge的数据进行处理

```bash
#写入配置文件
cat <<EOF > /etc/sysctl.d/kubernetes.conf
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
#生效配置文件
modprobe br_netfilter
```

#### 1.4 最终内核参数优化

```bash
cat << EOF >/etc/sysctl.d/kubernetes.conf
net.bridge.bridge-nf-call-iptables=1
net.bridge.bridge-nf-call-ip6tables=1
net.ipv4.ip_forward=1
net.ipv4.tcp_tw_recycle=0
vm.swappiness=0 # 禁止使用 swap 空间，只有当系统 OOM 时才允许使用它
vm.overcommit_memory=1 # 不检查物理内存是否够用
vm.panic_on_oom=0 # 开启 OOM
fs.inotify.max_user_instances=8192
fs.inotify.max_user_watches=1048576fs.file-max=52706963
fs.nr_open=52706963
net.ipv6.conf.all.disable_ipv6=1
net.netfilter.nf_conntrack_max=2310720
EOF
```

加载内核

```bash
sysctl -p /etc/sysctl.d/kubernetes.conf
```

#### 1.5 kube-proxy开启ipvs的前置条件

由于ipvs已经加入到了内核的主干，所以为kube-proxy开启ipvs的前提需要加载以下的内核模块：

```bash
cat > /etc/sysconfig/modules/ipvs.modules <<EOF
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4
EOF
chmod 755 /etc/sysconfig/modules/ipvs.modules
bash /etc/sysconfig/modules/ipvs.modules
lsmod|egrep "ip_vs|nf_conntrack_ipv4"
```

上面脚本创建了的/etc/sysconfig/modules/ipvs.modules文件，保证在节点重启后能自动加载所需模块。

各个节点上已经安装了ipset软件包与管理工具

```bash
yum install -y ipset ipvsadm
```

如果以上前提条件如果不满足，则即使kube-proxy的配置开启了ipvs模式，也会退回到iptables模式。

#### 1.6 调整系统时区

```bash
# 设置系统时区为中国/上海
timedatectl set-timezone Asia/Shanghai
# 将当前的 UTC 时间写入硬件时钟timedatectl set-local-rtc 0
# 重启依赖于系统时间的服务systemctl restart rsyslog
systemctl restart crond
```

#### 1.7 设置 rsyslogd 和 systemd journald

```bash
mkdir /var/log/journal # 持久化保存日志的目录
mkdir /etc/systemd/journald.conf.d
cat << EOF >/etc/systemd/journald.conf.d/99-prophet.conf
[Journal]
# 持久化保存到磁盘
Storage=persistent
# 压缩历史日志
Compress=yes
SyncIntervalSec=5m
RateLimitInterval=30s
RateLimitBurst=1000
# 最大占用空间 10G
SystemMaxUse=10G
# 单日志文件最大 200M
SystemMaxFileSize=200M
# 日志保存时间 2 周
MaxRetentionSec=2week
# 不将日志转发到 
syslogForwardToSyslog=no
EOF
systemctl restart systemd-journald
```

#### 1.8 升级系统内核为 4.44

CentOS 7.x 系统自带的 3.10.x 内核存在一些 Bugs，导致运行的 Docker、Kubernetes 不稳定 

```bash
rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm
```

 安装完成后检查 /boot/grub2/grub.cfg 中对应内核 menuentry 中是否包含 initrd16 配置，如果没有，再安装一次！

```bash
yum --enablerepo=elrepo-kernel install -y kernel-lt
```

查看新内核

```bash
cat /boot/grub2/grub.cfg |grep "^menuentry"|awk -F '[()]' 'NR==1{print $2}'
4.4.220-1.el7.elrepo.x86_64
```

设置开机从新内核启动

```bash
grub2-set-default 0
```

查看内核

```bash
uname -r
4.4.218-1.el7.elrepo.x86_64
```

#### 1.9 关闭 NUMA

备份grub

```bash
cp /etc/default/grub{,.bak}
```

在 GRUB_CMDLINE_LINUX 一行添加 `numa=off` 参数，如下所示：

```bash
GRUB_CMDLINE_LINUX="crashkernel=auto rd.lvm.lv=centos/root rhgb quiet numa=off"
```

加载内核

```bash
grub2-mkconfig -o /boot/grub2/grub.cfg
```

## 2. 集群基础设置

#### 2.1.1节点信息

| 节点角色 |   IP地址   | Hostname | CPU  | 内存 |                            server                            |
| :------: | :--------: | :------: | :--: | ---- | :----------------------------------------------------------: |
|          | 10.0.0.150 | k8s-vip  |  1   | 2G   |                                                              |
| master01 | 10.0.0.31  |    m1    |  1   | 2G   | etcd  apiserver scheduler controller-manager  kubectl  haproxy keepalived |
| master02 | 10.0.0.32  |    m2    |  1   | 2G   | etcd  apiserver scheduler controller-manager kubectl  haproxy keepalived |
| master03 | 10.0.0.33  |    m3    |  1   | 2G   | etcd  apiserver scheduler controller-manager kubectl  haproxy keepalived |
|  node01  | 10.0.0.41  |    n1    |  1   | 2G   |                  docker kubelet kube-proxy                   |
|  node02  | 10.0.0.42  |    n2    |  1   | 2G   |                  docker kubelet kube-proxy                   |
|  node03  | 10.0.0.43  |    n3    |  1   | 2G   |                  docker kubelet kube-proxy                   |

#### 2.1.2 网络配置信息

node网段：
**10.0.0.0/24**
service网段：
**10.96.0.0/24**
pod网段：
**172.16.0.0/16**

#### 2.2 hosts解析

```bash
cat << EOF >>/etc/hosts
10.0.0.31 m1
10.0.0.32 m2
10.0.0.33 m3
10.0.0.41 n1
10.0.0.42 n2
10.0.0.43 n3
EOF
```

#### 2.3 节点互信

```bash
#生成密钥
ssh-keygen -t dsa -f "/root/.ssh/id_dsa" -N "" -q
#将公钥作为认证信息
cat /root/.ssh/id_dsa.pub >/root/.ssh/authorized_keys
for n in m{2..3} n{1..3};do scp -r /root/.ssh $n:/root/;done
```

#### 2.4 设置环境变量

```bash
cat << EOF >>/root/.bash_profile
KUBE_APISERVER="https://10.0.0.100:8443"
TOKEN_ID=$(openssl rand -hex 3)
TOKEN_SECRET=$(openssl rand -hex 8)
AUTH_EXTRA_GROUPS="system:bootstrappers:default-node-token"
K8S_DIR=/opt/kubernetes
M_IP=(10.0.0.31 10.0.0.32 10.0.0.33)
N_IP=(10.0.0.41 10.0.0.42 10.0.0.43)
M_NAME=(m1 m2 m3)
ETCD_ENDPOINTS=etcd-m1=https://10.0.0.31:2380,etcd-m2=https://10.0.0.32:2380,etcd-m3=https://10.0.0.33:2380
SERVICE_CIDR=10.96.0.0/24
NODE_PORT_RANGE=30000-32767
CLUSTER_CIDR=172.16.0.0/16
ENCRYPTION_KEY=$(head -c 32 /dev/urandom | base64)
ETCD_DATA_DIR=/data/etcd/data
LOG_DIR=/data/kubernetes/logs
K8S_SSL=/etc/kubernetes/pki
K8S_CONF=/etc/kubernetes
ETCD_SERVER=https://10.0.0.31:2379,https://10.0.0.32:2379,https://10.0.0.33:2379
VIP=10.0.0.100
DNS=10.96.0.10
EOF
. ~/.bash_profile
```

#### 2.6 下载集群所需软件

[kubernetes](https://dl.k8s.io/v1.18.2/kubernetes-server-linux-amd64.tar.gz)
[etcd](https://github.com/etcd-io/etcd/releases/download/v3.4.7/etcd-v3.4.7-linux-amd64.tar.gz)
[cni](https://github.com/containernetworking/plugins/releases/download/v0.8.5/cni-plugins-linux-amd64-v0.8.5.tgz)
[flanneld](https://github.com/coreos/flannel/releases/download/v0.12.0/flannel-v0.12.0-linux-amd64.tar.gz)
[docker](https://download.docker.com/linux/static/stable/x86_64/docker-19.03.9.tgz)

**创建目录**

```bash
for n in m{1..3};do ssh $n "mkdir -p /etc/kubernetes/pki /opt/kubernetes/bin /root/.kube /data/kubernetes/logs/{kube-scheduler,kube-apiserver,kube-controller-manager}";done
for n in n{1..3};do ssh $n "mkdir -p /etc/kubernetes/{pki,manifests} /opt/cni/bin /etc/cni/net.d /data/kubernetes/logs/{kubelet,kube-proxy} /opt/kubernetes/bin/";done
```

**kubernetes解压推送各节点**

```bash
tar -zxvf kubernetes-server-linux-amd64.tar.gz 
cd kubernetes/server/bin/
#复制可执行文件到其余master节点
for n in m{1,2,3};do scp kubectl kube-apiserver kube-scheduler kube-controller-manager kube-proxy $n:${K8S_DIR}/bin/;done
#复制可执行文件到其余node节点
for n in n{1,2};do scp kubelet kube-proxy root@$n:${K8S_DIR}/bin/;done
```

**ETC解压推送各节点**

```bash
cd -
tar zxvf etcd-v3.4.9-linux-amd64.tar.gz
#复制可执行文件到其余master节点
for n in m{1..3};do rsync -av etcd-v3.4.9-linux-amd64/etcd $n:${K8S_DIR}/bin/;done
for n in m{1..3};do rsync -av etcd-v3.4.9-linux-amd64/etcdctl $n:/usr/local/bin/;done
```

**cfssl下载**

```bash
wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 -O /usr/local/bin/cfssl
wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 -O /usr/local/bin/cfssljson
chmod +x /usr/local/bin/cfssl*
```

**cni解压推送各节点**

```bash
cd -
for n in n{1..3};do scp cni-plugins-linux-amd64-v0.8.5.tgz $n:/root/ && ssh $n "tar -zxf cni-plugins-linux-amd64-v0.8.5.tgz -C /opt/cni/bin";done
```

**flanneld解压推送各节点**

```bash
cd - 
for n in n{1..3};do scp flannel-v0.12.0-linux-amd64.tar.gz $n:"${SOFT_DIR}" && ssh $n tar xf ${SOFT_DIR}/flannel-v0.12.0-linux-amd64.tar.gz -C ${K8S_DIR}/bin/;done
```

## 3. 安装docker(node节点）

### 3.1 yum安装

#### 3.1.1 安装依赖

```bash
yum install -y yum-utils device-mapper-persistent-data lvm2
```

#### 3.1.2 配置docker源

```bash
yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
yum makecache fast
rpm --import https://mirrors.aliyun.com/docker-ce/linux/centos/gpg
```

#### 3.1.3 查看docker版号

```bash
yum list docker-ce.x86_64  --showduplicates |sort -r
```

 Kubernetes 1.16当前支持的docker版本列表是 Docker版本1.13.1、17.03、17.06、17.09、18.06、18.09 

#### 3.1.4 安装 docker

```bash
yum makecache fast
yum install -y docker-ce-18.09.9-3.el7 
```

### 3.2 二进制安装

#### 3.2.1 下载地址

```bash
https://download.docker.com/linux/static/stable/x86_64/docker-19.03.9.tgz
```

#### 3.2.2 解压安装

```bash
tar zxvf docker-19.03.9.tgz
mv docker/* /usr/bin
```

#### 3.2.3 systemctl管理docker

```bash
cat << 'EOF' >/usr/lib/systemd/system/docker.service
[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target firewalld.service
Wants=network-online.target

[Service]
Type=notify
ExecStart=/usr/bin/dockerd
ExecReload=/bin/kill -s HUP $MAINPID
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity
TimeoutStartSec=0
Delegate=yes
KillMode=process
Restart=on-failure
StartLimitBurst=3
StartLimitInterval=60s

[Install]
WantedBy=multi-user.target
EOF
```

### 3.3 配置所有ip的数据包转发

```bash
#找到ExecStart=xxx，在这行下面加入一行，内容如下：(k8s的网络需要)
sed -i.bak '/ExecStart/a\ExecStartPost=/sbin/iptables -I FORWARD -s 0.0.0.0/0 -j ACCEPT' /lib/systemd/system/docker.service
```

### 3.4 docker基础优化

```bash
mkdir -p /etc/docker/
cat << EOF >/etc/docker/daemon.json
{
    "log-driver": "json-file",
    "graph":"/data/docker",
    "log-opts": { "max-size": "100m"},
    "exec-opts": ["native.cgroupdriver=systemd"],
    "registry-mirrors": ["https://s3w3uu4l.mirror.aliyuncs.com"],
    "insecure-registries":["http://harbor.wzxmt.com"],
    "max-concurrent-downloads": 10,
    "max-concurrent-uploads": 10,
    "storage-driver": "overlay2", 
    "storage-opts": ["overlay2.override_kernel_check=true"]
}
EOF
```

### 3.5 启动服务

```bash
systemctl daemon-reload && systemctl enable --now docker
```

## 4. 集群证书生成

#### 4.1 生成CA私钥和证书

```bash
cd $K8S_SSL
cat >ca-config.json << EOF
{"signing":{"default":{"expiry":"87600h"},"profiles":{"kubernetes":{"usages":["signing","key encipherment","server auth","client auth"],"expiry":"87600h"}}}}
EOF
cat > ca-csr.json << EOF 
{"CN": "kubernetes","key": {"algo": "rsa","size": 2048},"names":[{"C": "CN","ST": "BeiJing","L": "BeiJing","O": "kubernetes","OU": "k8s"}]}
EOF
cfssl gencert -initca ca-csr.json | cfssljson -bare ca
```

#### 4.2 生成Front proxy client 私钥和证书

```bash
cat >front-proxy-client-csr.json<<EOF
{"CN":"front-proxy-client","key":{"algo":"rsa","size":2048},"names":[{"C":"CN","ST":"BeiJing","L":"BeiJing","O":"system:masters","OU":"k8s"}]}
EOF
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes front-proxy-client-csr.json | cfssljson -bare front-proxy-client
```

#### 4.3 生成kube-apiserver私钥和证书

```bash
cat > apiserver-csr.json <<EOF 
{"CN":"apiserver","key":{"algo":"rsa","size":2048},"names":[{"C":"CN","ST":"BeiJing","L":"BeiJing","O":"kubernetes","OU":"k8s"}]}
EOF
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -hostname=10.96.0.1,10.0.0.1,172.16.0.1,127.0.0.1,kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster,kubernetes.default.svc.cluster.local,10.0.0.31,10.0.0.32,10.0.0.33,10.0.0.100 -profile=kubernetes apiserver-csr.json|cfssljson -bare apiserver
```

#### 4.4 生成kube-apiserver-kubelet-client 私钥和证书

```bash
cat > apiserver-kubelet-client-csr.json <<EOF 
{"CN":"kube-apiserver-kubelet-client","key":{"algo":"rsa","size":2048},"names":[{"C":"CN","ST":"BeiJing","L":"BeiJing","O":"system:masters","OU":"k8s"}]}
EOF
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes apiserver-kubelet-client-csr.json|cfssljson -bare apiserver-kubelet-client
```

#### 4.5 生成kube-proxy 私钥和证书

```bash
cat > kube-proxy-csr.json << EOF
{"CN":"system:kube-proxy","key":{"algo": "rsa","size":2048},"names":[{"C":"CN","L":"BeiJing","ST":"BeiJing","O":"kubernetes","OU":"k8s"}]}
EOF
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy
```

#### 4.6生成etcd私钥和证书

```bash 
cat > etcd-csr.json <<EOF 
{"CN":"etcd","key":{"algo":"rsa","size":2048},"names":[{"C":"CN","ST":"BeiJing","L":"BeiJing","O":"etcd","OU":"etcd"}]}
EOF
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -hostname=127.0.0.1,10.0.0.31,10.0.0.32,10.0.0.33 -profile=kubernetes etcd-csr.json|cfssljson -bare etcd
```

#### 4.7 创建 flanneld 证书和私钥

```bash
cat >flanneld-csr.json <<EOF
{"CN":"flanneld","key":{"algo":"rsa","size":2048},"names":[{"C":"CN","ST":"BeiJing","L":"BeiJing","O":"system","OU":"k8s"}]}
EOF
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes flanneld-csr.json | cfssljson -bare flanneld
```

#### 4.8 Service Account Key,api与pod间的认证

```bash
openssl genrsa -out sa.key 2048
openssl rsa -in sa.key -pubout -out sa.pub
```

删除证书请求

```bash
rm -f *.json *.csr
```

分发证书及配置文件相应节点

```bash
for n in m{1..3};do rsync -av ${K8S_SSL} root@$n:/etc/kubernetes/;done
for m in n{1..3};do rsync -av ${K8S_SSL}/ca.pem root@$m:${K8S_SSL}/;done
```

## 5. 生成kubeconfig认证文件

#### 5.1 scheduler-kubeconfig

```bash
kubectl config set-cluster kubernetes --certificate-authority=ca.pem --embed-certs=true --server=${KUBE_APISERVER} --kubeconfig=${K8S_CONF}/kube-scheduler.kubeconfig

kubectl config set-credentials system:kube-scheduler --client-certificate=apiserver-kubelet-client.pem --client-key=apiserver-kubelet-client-key.pem --embed-certs=true --kubeconfig=${K8S_CONF}/kube-scheduler.kubeconfig

kubectl config set-context system:kube-scheduler@kubernetes --cluster=kubernetes --user=system:kube-scheduler --kubeconfig=${K8S_CONF}/kube-scheduler.kubeconfig

kubectl config use-context system:kube-scheduler@kubernetes --kubeconfig=${K8S_CONF}/kube-scheduler.kubeconfig
```

#### 5.2 controller-manager-kubeconfig

```bash
kubectl config set-cluster kubernetes --certificate-authority=ca.pem --embed-certs=true --server=${KUBE_APISERVER} --kubeconfig=${K8S_CONF}/kube-controller-manager.kubeconfig

kubectl config set-credentials system:kube-controller-manager --client-certificate=apiserver-kubelet-client.pem --client-key=apiserver-kubelet-client-key.pem --embed-certs=true --kubeconfig=${K8S_CONF}/kube-controller-manager.kubeconfig

kubectl config set-context system:kube-controller-manager@kubernetes --cluster=kubernetes --user=system:kube-controller-manager --kubeconfig=${K8S_CONF}/kube-controller-manager.kubeconfig

kubectl config use-context system:kube-controller-manager@kubernetes --kubeconfig=${K8S_CONF}/kube-controller-manager.kubeconfig
```

#### 5.3 kubelet bootstrapping kubeconfig

```bash
kubectl config set-cluster kubernetes --certificate-authority=ca.pem --embed-certs=true --server=${KUBE_APISERVER} --kubeconfig=${K8S_CONF}/bootstrap-kubelet.kubeconfig

kubectl config set-credentials tls-bootstrap-token-user --token=${TOKEN_ID}.${TOKEN_SECRET} --kubeconfig=${K8S_CONF}/bootstrap-kubelet.kubeconfig

kubectl config set-context default --cluster=kubernetes --user=tls-bootstrap-token-user --kubeconfig=${K8S_CONF}/bootstrap-kubelet.kubeconfig

kubectl config use-context default --kubeconfig=${K8S_CONF}/bootstrap-kubelet.kubeconfig
```

#### 5.4 proxy-kubeconfig

```bash
kubectl config set-cluster kubernetes --certificate-authority=ca.pem --embed-certs=true --server=${KUBE_APISERVER} --kubeconfig=${K8S_CONF}/kube-proxy.kubeconfig

kubectl config set-credentials kube-proxy --client-certificate=kube-proxy.pem --client-key=kube-proxy-key.pem --embed-certs=true --kubeconfig=${K8S_CONF}/kube-proxy.kubeconfig

kubectl config set-context default --cluster=kubernetes --user=kube-proxy --kubeconfig=${K8S_CONF}/kube-proxy.kubeconfig

kubectl config use-context default --kubeconfig=${K8S_CONF}/kube-proxy.kubeconfig
```

#### 5.5 admin-kubeconfig

```bash
kubectl config set-cluster kubernetes --certificate-authority=ca.pem --embed-certs=true --server=${KUBE_APISERVER} --kubeconfig=${K8S_CONF}/admin.kubeconfig

kubectl config set-credentials admin --client-certificate=apiserver-kubelet-client.pem --client-key=apiserver-kubelet-client-key.pem --embed-certs=true --kubeconfig=${K8S_CONF}/admin.kubeconfig

kubectl config set-context admin@kubernetes --cluster=kubernetes --user=admin --kubeconfig=${K8S_CONF}/admin.kubeconfig

kubectl config use-context admin@kubernetes --kubeconfig=${K8S_CONF}/admin.kubeconfig
```

分发配置

```bash
for n in m{1..3};do scp ${K8S_CONF}/*.kubeconfig $n:${K8S_CONF}/;done
for n in m{1..3};do ssh root@$n cp ${K8S_CONF}/admin.kubeconfig /root/.kube/config;done
for n in n{1..3};do scp ${K8S_CONF}/{kube-proxy.kubeconfig,bootstrap-kubelet.kubeconfig} $n:${K8S_CONF}/;done
```

## 5 systemctl管理etcd

```bash
cd $HOME
cat << 'AOF' >etcd-template.sh
#!/bin/bash
source /root/.bash_profile
for ((i=0;i<3;i++))
do
  NODE_NAME=${M_NAME[i]}
  NODE_IP=${M_IP[i]}
cat << EOF >etcd.service 
[Unit]
Description=Etcd Server
After=neCNork.target
After=network-online.target
Wants=network-online.target
[Service]
Type=notify
ExecStart=${K8S_DIR}/bin/etcd \\
--data-dir=${ETCD_DATA_DIR} \\
--logger=zap \\
--name=etcd-${NODE_NAME} \\
--cert-file=${K8S_SSL}/etcd.pem \\
--key-file=${K8S_SSL}/etcd-key.pem \\
--trusted-ca-file=${K8S_SSL}/ca.pem \\
--peer-cert-file=${K8S_SSL}/etcd.pem \\
--peer-key-file=${K8S_SSL}/etcd-key.pem \\
--peer-trusted-ca-file=${K8S_SSL}/ca.pem \\
--listen-peer-urls=https://${NODE_IP}:2380 \\
--initial-advertise-peer-urls=https://${NODE_IP}:2380 \\
--listen-client-urls=https://${NODE_IP}:2379,http://127.0.0.1:2379 \\
--advertise-client-urls=https://${NODE_IP}:2379 \\
--initial-cluster ${ETCD_ENDPOINTS} \\
--initial-cluster-token=etcd-cluster \\
--initial-cluster-state=new \\
--auto-compaction-mode=periodic \\
--auto-compaction-retention=1 \\
--max-request-bytes=33554432 \\
--quota-backend-bytes=6442450944 \\
--heartbeat-interval=250 \\
--election-timeout=2000
Restart=on-failure
LimitNOFILE=65536
[Install]
WantedBy=multi-user.target
EOF
rsync -av etcd.service ${NODE_IP}:/usr/lib/systemd/system/
done
   rm -f etcd*
AOF
```

#### 5.2 分发配置文件

```bash
sh etcd-template.sh
```

#### 5.3 启动etcd

```bash
for n in m{1..3};do ssh $n "systemctl daemon-reload && systemctl enable --now etcd";done
```

#### 5.4 检查etcd集群状态

```bash
etcdctl --cacert=${K8S_SSL}/ca.pem \
--cert=${K8S_SSL}/etcd.pem \
--key=${K8S_SSL}/etcd-key.pem \
--endpoints="${ETCD_SERVER}" endpoint health

https://10.0.0.31:2379 is healthy: successfully committed proposal: took = 10.737332ms
https://10.0.0.32:2379 is healthy: successfully committed proposal: took = 11.069201ms
https://10.0.0.33:2379 is healthy: successfully committed proposal: took = 11.845976ms
```

## 6. apiserver集群部署

#### 6.1 创建审计策略

```bash
cat << EOF >audit-policy.yaml
apiVersion: audit.k8s.io/v1beta1 # This is required.
kind: Policy
# Don't generate audit events for all requests in RequestReceived stage.
omitStages:
  - "RequestReceived"
rules:
  # Log pod changes at RequestResponse level
  - level: RequestResponse
    resources:
    - group: ""
      # Resource "pods" doesn't match requests to any subresource of pods,
      # which is consistent with the RBAC policy.
      resources: ["pods"]
  # Log "pods/log", "pods/status" at Metadata level
  - level: Metadata
    resources:
    - group: ""
      resources: ["pods/log", "pods/status"]
  # Don't log requests to a configmap called "controller-leader"
  - level: None
    resources:
    - group: ""
      resources: ["configmaps"]
      resourceNames: ["controller-leader"]
  # Don't log watch requests by the "system:kube-proxy" on endpoints or services
  - level: None
    users: ["system:kube-proxy"]
    verbs: ["watch"]
    resources:
    - group: "" # core API group
      resources: ["endpoints", "services"]
  # Don't log authenticated requests to certain non-resource URL paths.
  - level: None
    userGroups: ["system:authenticated"]
    nonResourceURLs:
    - "/api*" # Wildcard matching.
    - "/version"
  # Log the request body of configmap changes in kube-system.
  - level: Request
    resources:
    - group: "" # core API group
      resources: ["configmaps"]
    # This rule only applies to resources in the "kube-system" namespace.
    # The empty string "" can be used to select non-namespaced resources.
    namespaces: ["kube-system"]
  # Log configmap and secret changes in all other namespaces at the Metadata level.
  - level: Metadata
    resources:
    - group: "" # core API group
      resources: ["secrets", "configmaps"]
  # Log all other resources in core and extensions at the Request level.
  - level: Request
    resources:
    - group: "" # core API group
    - group: "extensions" # Version of group should NOT be included.
  # A catch-all rule to log all other requests at the Metadata level.
  - level: Metadata
    # Long-running requests like watches that fall under this rule will not
    # generate an audit event in RequestReceived.
    omitStages:
      - "RequestReceived"
EOF
```

#### 6.2 创建加密配置文件

```bash
cat >encryption-config.yaml <<EOF
kind: EncryptionConfig
apiVersion: v1
resources:
  - resources:
      - secrets
    providers:
      - aescbc:
          keys:
            - name: key1
              secret: ${ENCRYPTION_KEY}
      - identity: {}
EOF
```

#### 6.1 systemctl管理apiserver

```bash
cat << 'AOF' >apiserver-template.sh
#!/bin/bash
source /root/.bash_profile
for ((i=0;i<3;i++))
do
  NODE_NAME=${M_NAME[i]}
  NODE_IP=${M_IP[i]}
cat << EOF >kube-apiserver.service
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes
After=network.target
[Service]
ExecStart=${K8S_DIR}/bin/kube-apiserver \\
  --audit-log-maxage=30 \\
  --audit-log-maxbackup=10 \\
  --audit-log-maxsize=100 \\
  --service-node-port-range=${NODE_PORT_RANGE} \\
  --audit-policy-file ${K8S_CONF}/audit-policy.yaml \\
  --audit-log-path ${LOG_DIR}/kube-apiserver/audit-log \\
  --encryption-provider-config=${K8S_CONF}/encryption-config.yaml \\
  --allow-privileged=true \\
  --authorization-mode=Node,RBAC \\
  --client-ca-file=${K8S_SSL}/ca.pem \\
  --enable-admission-plugins=NodeRestriction \\
  --enable-bootstrap-token-auth=true \\
  --etcd-cafile=${K8S_SSL}/ca.pem \\
  --etcd-certfile=${K8S_SSL}/etcd.pem \\
  --etcd-keyfile=${K8S_SSL}/etcd-key.pem \
  --etcd-servers=${ETCD_SERVER} \\
  --kubelet-client-certificate=${K8S_SSL}/apiserver-kubelet-client.pem \\
  --kubelet-client-key=${K8S_SSL}/apiserver-kubelet-client-key.pem \\
  --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname \\
  --proxy-client-cert-file=${K8S_SSL}/front-proxy-client.pem \\
  --proxy-client-key-file=${K8S_SSL}/front-proxy-client-key.pem \\
  --requestheader-allowed-names=front-proxy-client \\
  --requestheader-client-ca-file=${K8S_SSL}/ca.pem \\
  --requestheader-extra-headers-prefix=X-Remote-Extra- \\
  --requestheader-group-headers=X-Remote-Group \\
  --requestheader-username-headers=X-Remote-User \\
  --enable-aggregator-routing=true \\
  --secure-port=6443 \\
  --service-account-key-file=${K8S_SSL}/sa.pub \\
  --service-cluster-ip-range 10.96.0.0/16 \\
  --tls-cert-file=${K8S_SSL}/apiserver.pem \\
  --tls-private-key-file=${K8S_SSL}/apiserver-key.pem \\
  --log-dir ${LOG_DIR}/kube-apiserver \\
  --alsologtostderr=true \\
  --logtostderr=false \\
  --v=2 \\
  --anonymous-auth=false \\
  --apiserver-count=3 \\
  --audit-dynamic-configuration \\
  --audit-log-truncate-enabled \\
  --default-not-ready-toleration-seconds=360 \\
  --default-unreachable-toleration-seconds=360 \\
  --default-watch-cache-size=200 \\
  --delete-collection-workers=2 \\
  --feature-gates=DynamicAuditing=true \\
  --kubelet-https=true \\
  --kubelet-timeout=10s \\
  --max-mutating-requests-inflight=2000 \\
  --max-requests-inflight=4000 \\
  --runtime-config=api/all=true 
Restart=on-failure
RestartSec=10s
LimitNOFILE=65535
[Install]
WantedBy=multi-user.target
EOF
rsync -av encryption-config.yaml audit-policy.yaml  ${NODE_IP}:${K8S_CONF}/
rsync -av kube-apiserver.service ${NODE_IP}:/usr/lib/systemd/system/
done
   rm -f *apiserver* encryption-config.yaml audit-policy.yaml
AOF
```

#### 6.2 分发配置文件

```bash
sh apiserver-template.sh
```

#### 6.3 启动服务

```bash
for n in m{1..3};do ssh $n "systemctl daemon-reload && systemctl enable --now kube-apiserver";done
```

#### 6.4 检查kube-apiserver监听的端口

```bash
[root@m1 ~]# ss -lntup|grep 6443
tcp    LISTEN     0      128    10.0.0.31:6443                  *:*                   users:(("kube-apiserver",pid=18425,fd=6))
```

## 7. apiserver高可用部署

#### 7.1 各master节点安装haproxy keepalived

```bash
for m in m{1,2,3};do ssh root@$m yum -y install haproxy keepalived;done
```

#### 7.2 修改配置文件

注意:keepalived配置文件，其余节点修改state为BACKUP，priority小于主节点即可；检查网卡名称并修改

```bash
cat<< EOF >/etc/keepalived/keepalived.conf
vrrp_script chk_apiserver {
        script "/etc/keepalived/check_apiserver.sh"
        interval 4
        weight 60  
}
vrrp_instance VI_1 {
    #state MASTER  
    state BACKUP
    interface eth0
    virtual_router_id 51
    priority 130
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    track_script {
        chk_apiserver
    }
    virtual_ipaddress {
        10.0.0.100
    }
}
EOF
cat << 'EOF' >/etc/keepalived/check_apiserver.sh
#!/bin/bash
flag=$(ps -ef|grep -v grep|grep -w 'kube-apiserver' &>/dev/null;echo $?)
if [[ $flag != 0 ]];then
        echo "kube-apiserver is down,close the keepalived"
        systemctl stop keepalived
fi
EOF
cat > /etc/haproxy/haproxy.cfg << EOF 
global
    log         127.0.0.1 local2
    chroot      /var/lib/haproxy
    pidfile     /var/run/haproxy.pid
    maxconn     4000
    user        haproxy
    group       haproxy
    daemon
    stats socket /var/lib/haproxy/stats
#---------------------------------------------------------------------
defaults
    mode                    http
    log                     global
    option                  httplog
    option                  dontlognull
    option http-server-close
    option forwardfor       except 127.0.0.0/8
    option                  redispatch
    retries                 3
    timeout http-request    10s
    timeout queue           1m
    timeout connect         10s
    timeout client          1m
    timeout server          1m
    timeout http-keep-alive 10s
    timeout check           10s
    maxconn                 3000
#---------------------------------------------------------------------
frontend  k8s-api 
   bind *:8443
   mode tcp
   default_backend             apiserver
#---------------------------------------------------------------------
backend apiserver
    balance     roundrobin
    mode tcp
    server  m1 10.0.0.31:6443 check weight 1 maxconn 2000 check inter 2000 rise 2 fall 3
    server  m2 10.0.0.32:6443 check weight 1 maxconn 2000 check inter 2000 rise 2 fall 3
    server  m3 10.0.0.33:6443 check weight 1 maxconn 2000 check inter 2000 rise 2 fall 3
EOF
```

#### 7.3 修改state与priority,并启动服务

```bash
chmod 644 /etc/keepalived/keepalived.conf && chmod +x /etc/keepalived/check_apiserver.sh && systemctl enable --now haproxy keepalived
#查看VIP是否工作正常
ping 10.0.0.100 -c 3
```

#### 7.5 查看apiserver集群健康状况

```bash
[root@m1 pki]# kubectl cluster-info
Kubernetes master is running at https://10.0.0.100:8443

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
```

## 8. scheduler 集群部署

#### 8.1 systemctl管理scheduler

```bash
cat << 'AOF' >scheduler-template.sh
#!/bin/bash
source /root/.bash_profile
for ((i=0;i<3;i++))
do
  NODE_IP=${M_IP[i]}
cat << EOF >kube-scheduler.service
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=${K8S_DIR}/bin/kube-scheduler \\
  --kubeconfig=${K8S_CONF}/kube-scheduler.kubeconfig \\
  --leader-elect=true \\
  --client-ca-file=${K8S_SSL}/ca.pem \\
  --requestheader-allowed-names=front-proxy-client \\
  --requestheader-client-ca-file=${K8S_SSL}/ca.pem \\
  --requestheader-extra-headers-prefix=X-Remote-Extra- \\
  --requestheader-group-headers=X-Remote-Group \\
  --requestheader-username-headers=X-Remote-User \\
  --alsologtostderr=true \\
  --logtostderr=false \\
  --log-dir ${LOG_DIR}/kube-scheduler \\
  --v=2
RestartSec=10s
LimitNOFILE=65535
Restart=on-failure
[Install]
WantedBy=multi-user.target
EOF
rsync -av kube-scheduler.service ${NODE_IP}:/usr/lib/systemd/system/
done
   rm -f *scheduler*
AOF
```

#### 8.2 分发配置文件

```bash
sh scheduler-template.sh
```

#### 8.3 启动服务

```bash
for n in m{1..3};do ssh $n "systemctl daemon-reload && systemctl enable --now kube-scheduler";done
```

#### 8.4 查看服务状态

```bash
systemctl status kube-scheduler.service
```

#### 8.5 查看leader信息

```bash
kubectl get ep kube-scheduler -n kube-system -o yaml
```

## 9. controller manager 集群部署

#### 9.1 systemctl管理controller-manager

```bash
cat << 'AOF' >controller-manager-template.sh
#!/bin/bash
source /root/.bash_profile
for ((i=0;i<3;i++))
do
  NODE_NAME=${M_NAME[i]}
  NODE_IP=${M_IP[i]}
cat << EOF >kube-controller-manager.service 
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/kubernetes/kubernetes
After=network.target
[Service]
ExecStart=${K8S_DIR}/bin/kube-controller-manager \\
  --allocate-node-cidrs=true \\
  --authentication-kubeconfig=${K8S_CONF}/kube-controller-manager.kubeconfig \\
  --authorization-kubeconfig=${K8S_CONF}/kube-controller-manager.kubeconfig \\
  --client-ca-file=${K8S_SSL}/ca.pem \
  --cluster-cidr=${CLUSTER_CIDR} \\
  --cluster-name=kubernetes \\
  --cluster-signing-cert-file=${K8S_SSL}/ca.pem \\
  --cluster-signing-key-file=${K8S_SSL}/ca-key.pem \\
  --controllers=*,bootstrapsigner,tokencleaner \\
  --kubeconfig=${K8S_CONF}/kube-controller-manager.kubeconfig \\
  --leader-elect=true \\
  --node-cidr-mask-size=24 \\
  --requestheader-allowed-names=front-proxy-client \\
  --requestheader-client-ca-file=${K8S_SSL}/ca.pem \\
  --requestheader-extra-headers-prefix=X-Remote-Extra- \\
  --requestheader-group-headers=X-Remote-Group \\
  --requestheader-username-headers=X-Remote-User \\
  --root-ca-file=${K8S_SSL}/ca.pem \\
  --service-account-private-key-file=${K8S_SSL}/sa.key \
  --cluster-cidr=${CLUSTER_CIDR} \\
  --use-service-account-credentials=true \\
  --horizontal-pod-autoscaler-use-rest-clients=true \\
  --feature-gates=RotateKubeletServerCertificate=true \\
  --log-dir=${LOG_DIR}/kube-controller-manager \\
  --alsologtostderr=true \\
  --logtostderr=false \\
  --v=2 \
  --horizontal-pod-autoscaler-sync-period=10s \
  --concurrent-deployment-syncs=10 \
  --concurrent-gc-syncs=30 \
  --node-cidr-mask-size=24 \
  --pod-eviction-timeout=6m \
  --terminated-pod-gc-threshold=10000 \
  --experimental-cluster-signing-duration=87600h0m0s
Restart=always
RestartSec=10s
[Install]
WantedBy=multi-user.target
EOF
rsync -av kube-controller-manager.service ${NODE_IP}:/usr/lib/systemd/system/
done
   rm -f *controller*
AOF
```

#### 9.3 分发配置文件

```bash
sh controller-manager-template.sh
```

#### 9.4 启动服务

```bash
for n in m{1..3};do ssh $n "systemctl daemon-reload && systemctl enable --now kube-controller-manager";done
```

#### 9.5 查看服务状态

```bash
systemctl status kube-controller-manager.service
```

#### 9.6 查看leader信息

```bash
kubectl get ep kube-controller-manager -n kube-system -o yaml
```

查看集群状态

```bash
[root@master01 pki]# kubectl get cs
NAME                 STATUS    MESSAGE             ERROR
scheduler            Healthy   ok
controller-manager   Healthy   ok
etcd-1               Healthy   {"health":"true"}
etcd-2               Healthy   {"health":"true"}
etcd-0               Healthy   {"health":"true"}
```

## 10. TLS bootstrapping认证

每个节点的kubelet都必须使用kube-apiserver的CA的凭证后,才能与kube-apiserver进行沟通,而该过程需要手动针对每台节点单独签署凭证是一件繁琐的事情,且一旦节点增加会延伸出管理不易问题;而TLS bootstrapping目标就是解决该问题,通过让kubelet先使用一个预定低权限使用者连接到kube-apiserver,然后在对kube-apiserver申请凭证签署,当授权Token一致时,Node节点的kubelet凭证将由kube-apiserver动态签署提供。

TLS bootstraping 工作流程：

![img](https://k8s-1252881505.cos.ap-beijing.myqcloud.com/k8s-1/bootstrap-token.png)

#### 10.1 建立TLS bootstrap secret来提供自动签证使用

```bash
cat << EOF >bootstrap-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: bootstrap-token-${TOKEN_ID}
  namespace: kube-system
type: bootstrap.kubernetes.io/token
stringData:
  description: "The bootstrap token for k8s."
  token-id: ${TOKEN_ID}
  token-secret: ${TOKEN_SECRET}
  expiration: 2029-07-16T00:00:00Z
  usage-bootstrap-authentication: "true"
  usage-bootstrap-signing: "true"
  auth-extra-groups: ${AUTH_EXTRA_GROUPS}
EOF
kubectl apply -f  bootstrap-secret.yaml
```

#### 10.2 将自定义的auth-extra-groups绑定角色system:node-bootstrapper

```bash
kubectl create clusterrolebinding kubelet-bootstrap \
--clusterrole system:node-bootstrapper \
--group ${AUTH_EXTRA_GROUPS}
```

#### 10.3 将自定义的auth-extra-groups绑定角色,实现自动签署证书请求

```bash
kubectl create clusterrolebinding node-autoapprove-bootstrap \
--clusterrole system:certificates.k8s.io:certificatesigningrequests:nodeclient \
--group ${AUTH_EXTRA_GROUPS}
```

#### 10.4 将system:node绑定角色,实现自动刷新node节点过期证书

```bash
kubectl create clusterrolebinding node-autoapprove-certificate-rotation \
--clusterrole system:certificates.k8s.io:certificatesigningrequests:selfnodeclient \
--group system:node
```

## 11. kubelet部署

#### 11.1 系统配置文件

```bash
cat << 'AOF' >kubelet-template.sh
#!/bin/bash
source /root/.bash_profile
for ((i=0;i<3;i++))
do
  NODE_IP=${N_IP[i]}
cat > kubelet-conf.yml << EOF 
address: 0.0.0.0
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    cacheTTL: 2m0s
    enabled: true
  x509:
    clientCAFile: ${K8S_SSL}/ca.pem
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: 5m0s
    cacheUnauthorizedTTL: 30s
cgroupDriver: systemd
cgroupsPerQOS: true
clusterDNS:
- ${DNS}
clusterDomain: cluster.local
configMapAndSecretChangeDetectionStrategy: Watch
containerLogMaxFiles: 5
containerLogMaxSize: 10Mi
contentType: application/vnd.kubernetes.protobuf
cpuCFSQuota: true
cpuCFSQuotaPeriod: 100ms
cpuManagerPolicy: none
cpuManagerReconcilePeriod: 10s
enableControllerAttachDetach: true
enableDebuggingHandlers: true
enforceNodeAllocatable:
- pods
eventBurst: 10
eventRecordQPS: 5
evictionHard:
  imagefs.available: 15%
  memory.available: 100Mi
  nodefs.available: 10%
  nodefs.inodesFree: 5%
evictionPressureTransitionPeriod: 5m0s
failSwapOn: true
fileCheckFrequency: 20s
hairpinMode: promiscuous-bridge
healthzBindAddress: 127.0.0.1
healthzPort: 10248
httpCheckFrequency: 20s
imageGCHighThresholdPercent: 85
imageGCLowThresholdPercent: 80
imageMinimumGCAge: 2m0s
iptablesDropBit: 15
iptablesMasqueradeBit: 14
kind: KubeletConfiguration
kubeAPIBurst: 10
kubeAPIQPS: 5
makeIPTablesUtilChains: true
maxOpenFiles: 1000000
maxPods: 110
nodeLeaseDurationSeconds: 40
nodeStatusReportFrequency: 1m0s
nodeStatusUpdateFrequency: 10s
oomScoreAdj: -999
podPidsLimit: -1
port: 10250
registryBurst: 10
registryPullQPS: 5
resolvConf: /etc/resolv.conf
rotateCertificates: true
runtimeRequestTimeout: 2m0s
serializeImagePulls: true
staticPodPath: ${K8S_CONF}/manifests
streamingConnectionIdleTimeout: 4h0m0s
syncFrequency: 1m0s
volumeStatsAggPeriod: 1m0s
EOF
cat << EOF >kubelet.service
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/kubernetes/kubernetes
After=docker.service
Requires=docker.service
[Service]
ExecStart=${K8S_DIR}/bin/kubelet \\
  --cgroup-driver=systemd \\
  --runtime-cgroups=/systemd/system.slice \\
  --kubelet-cgroups=/systemd/system.slice \\
  --bootstrap-kubeconfig=${K8S_CONF}/bootstrap-kubelet.kubeconfig \\
  --kubeconfig=${K8S_CONF}/kubelet.kubeconfig \\
  --config=${K8S_CONF}/kubelet-conf.yml \\
  --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.1 \\
  --network-plugin=cni \\
  --cni-conf-dir=/etc/cni/net.d \\
  --cni-bin-dir=/opt/cni/bin \\
  --cert-dir=${K8S_SSL} \\
  --log-dir=${LOG_DIR}/kubelet \\
  --image-pull-progress-deadline=15m \\
  --alsologtostderr=true \\
  --logtostderr=false \\
  --feature-gates=RotateKubeletClientCertificate=true,RotateKubeletServerCertificate=true \\
  --rotate-certificates \\
  --v=2
Restart=always
RestartSec=10s
[Install]
WantedBy=multi-user.target
EOF
rsync -av kubelet-conf.yml ${NODE_IP}:${K8S_CONF}/
rsync -av kubelet.service ${NODE_IP}:/usr/lib/systemd/system/
done
   rm -f kubelet*
AOF
```

#### 11.2 分发配置文件

```
sh kubelet-template.sh
```

#### 11.3 启动服务

```bash
for n in n{1..3};do ssh $n "systemctl daemon-reload && systemctl enable --now kubelet";done
```

## 12. kube-proxy部署

#### 12.1  systemctl管理kube-proxy

```bash
cat << 'AOF' >proxy-template.sh
#!/bin/bash
source /root/.bash_profile
for ((i=0;i<3;i++))
do
  NODE_IP=${N_IP[i]}
cat << EOF >kube-proxy.service
[Unit]
Description=Kubernetes Kube Proxy
Documentation=https://github.com/kubernetes/kubernetes
After=network.target
[Service]
ExecStart=${K8S_DIR}/bin/kube-proxy \\
  --bind-address=${NODE_IP} \\
  --kubeconfig=${K8S_CONF}/kube-proxy.kubeconfig \\
  --feature-gates=ServiceTopology=true,EndpointSlice=true \\
  --masquerade-all=true \\
  --proxy-mode=ipvs \\
  --ipvs-min-sync-period=5s \\
  --ipvs-sync-period=5s \\
  --ipvs-scheduler=rr \\
  --cluster-cidr=${CLUSTER_CIDR} \\
  --metrics-bind-address=${NODE_IP}:10249 \\
  --healthz-bind-address=${NODE_IP}:10256
  --alsologtostderr=true \\
  --logtostderr=false \\
  --log-dir=${LOG_DIR}/kubelet/kube-proxy \\
  --v=2
Restart=always
RestartSec=10s
[Install]
WantedBy=multi-user.target
EOF
rsync -av kube-proxy.service ${NODE_IP}:/usr/lib/systemd/system/
done
   rm -f *proxy*
AOF
```

#### 12.2 分发配置文件

```
sh kube-proxy-template.sh
```

#### 12.3 启动服务

```bash
for n in n{1..3};do ssh $n "systemctl daemon-reload && systemctl enable --now kube-proxy";done
```

#### 12.4 查看节点状态

```bash
[root@m1 ~]# kubectl get node
NAME     STATUS     ROLES    AGE   VERSION
n1   NotReady   <none>   7s    v1.18.9
n2   NotReady   <none>   4s    v1.18.9
```

#### 12.5 node节点确认使用IPVS模式

```bash
[root@m1 ~]# curl localhost:10249/proxyMode
ipvs
```

## 14. flanneld网络部署

kubernetes 要求集群内各节点(包括 master 节点)能通过 Pod 网段互联互通。flannel 使用 vxlan 技术为各节点创建一个可以互通的 Pod 网络，使用的端口为 UDP 8472。 第一次启动时，从 etcd 获取配置的 Pod 网段信息，为本节点分配一个未使用的地址段，然后创建flannel.1网络接口。flannel 将分配给自己的 Pod 网段信息写入 /run/flannel/docker 文件，docker 后续使用这个文件中的环境变量设置 docker0 网桥，从而从这个地址段为本节点的所有 Pod 容器分配 IP。

flanneld并不简单，它上连etcd，利用etcd来管理可分配的IP地 址段资源，同时监控etcd中每个Pod的实际地址，并在内存中建立了一 个Pod节点路由表；它下连docker0和物理网络，使用内存中的Pod节点路由表，将docker0发给它的数据包包装起来，利用物理网络的连接将 数据包投递到目标flanneld上，从而完成Pod到Pod之间的直接地址通信。

#### 14.1 向 etcd 写入集群 Pod 网段信息

```bash
ETCDCTL_API=2  etcdctl \
--ca-file=${K8S_SSL}/ca.pem \
--cert-file=${K8S_SSL}/flanneld.pem \
--key-file=${K8S_SSL}/flanneld-key.pem \
--endpoints="https://10.0.0.31:2379"  \
set /kubernetes/network/config \
'{"Network" : "172.16.0.0/16","SubnetLen": 24, "Backend": {"Type" : "vxlan"}}'
```

,注意：本步骤只需执行一次,flanneld 当前版本 (v0.12.0) 不支持 etcd v3，故使用 etcd v2 API 写入配置 key 和网段数据

#### 14.2 查看写入Pod网段信息

```bash
ETCDCTL_API=2 etcdctl \
--ca-file=${K8S_SSL}/ca.pem \
--cert-file=${K8S_SSL}/flanneld.pem \
--key-file=${K8S_SSL}/flanneld-key.pem \
--endpoints="https://10.0.0.31:2379"  \
get /kubernetes/network/config
```

#### 14.3 systemctl管理flanneld

```bash
cat << 'AOF' >flanneld-template.sh
#!/bin/bash
source /root/.bash_profile
for ((i=0;i<3;i++))
do
  NODE_IP=${N_IP[i]}
cat << EOF >flanneld.service
[Unit]
Description=Flanneld overlay address etcd agent
After=network.target
Before=docker.service
[Service]
Type=notify
ExecStart=${K8S_DIR}/bin/flanneld \\
  -etcd-cafile=${K8S_SSL}/ca.pem \\
  -etcd-certfile=${K8S_SSL}/flanneld.pem \\
  -etcd-keyfile=${K8S_SSL}/flanneld-key.pem \\
  -etcd-endpoints=${ETCD_SERVER} \\
  -etcd-prefix=/kubernetes/network \\
  -ip-masq
ExecStartPost=/usr/local/bin/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/docker
Restart=on-failure
[Install]
WantedBy=multi-user.target
RequiredBy=docker.service
EOF
cat<< EOF >10-flannel.conflist
{
  "name": "cbr0",
  "cniVersion": "0.3.1",
  "plugins": [
    {
      "type": "flannel",
      "delegate": {
        "hairpinMode": true,
        "isDefaultGateway": true
      }
    },
    {
      "type": "portmap",
      "capabilities": {
        "portMappings": true
      }
    }
  ]
}
EOF
rsync -av flanneld.service ${NODE_IP}:/usr/lib/systemd/system/
rsync -av 10-flannel.conflist ${NODE_IP}:/etc/cni/net.d/
done
   rm -f *flannel*
AOF
```

#### 14.4 分发配置

```bash
sh flanneld-template.sh
```

#### 14.5 启动flanneld服务并验证各节点flannel

```bash
for n in n{1..3};do ssh $n "systemctl enable --now flannel.service";done
for n in n{1..3};do ssh $n "ip a s flannel.1|grep -w inet";done
for n in n{1..3};do ssh $n ping -c 1 `ip a s flannel.1|grep -w inet|awk -F '[/ ]+' '{print $3}'` 2>/dev/null;done
```

#### 14.7 查看node状态

```bash
[root@master01 k8s]# kubectl get node
NAME       STATUS   ROLES    AGE   VERSION
n1     Ready    <none>   18m   v1.18.0
n2     Ready    <none>   18m   v1.18.0
n3     Ready    <none>   18m   v1.18.0
```

## 15. 污点配置

当我们的master也安装了kubelet与kube-proxy时，需要进行相应的污点配置。如：

```bash
[root@master01 k8s]# kubectl get node
NAME       STATUS   ROLES    AGE   VERSION
m1     Ready    <none>   18m   v1.18.0
m2     Ready    <none>   18m   v1.18.0
m3     Ready    <none>   18m   v1.18.0
n1     Ready    <none>   18m   v1.18.0
n2     Ready    <none>   18m   v1.18.0
n3     Ready    <none>   18m   v1.18.0
```

#### 15.1 设定master节点加上污点Taint(注意这块只能是主机名)

```bash
for n in m{1..3};do kubectl taint nodes $n node-role.kubernetes.io/master="":NoSchedule;done
```

#### 15.2 设置label role

```bash
for n in m{1..3};do kubectl label node $n node-role.kubernetes.io/master=;done
for n in n{1..3};do kubectl label node $n node-role.kubernetes.io/node=;done
#删除role
kubectl label node node01 node-role.kubernetes.io/node-
```

#### 15.3 查看node状态

```bash
[root@m1 ~]# kubectl get node
NAME     STATUS   ROLES    AGE   VERSION
m1   Ready    master   80m   v1.18.0
m2   Ready    master   80m   v1.18.0
m3   Ready    master   80m   v1.18.0
n1   Ready    node     65m   v1.18.0
n2   Ready    node     65m   v1.18.0
n3   Ready    node     65m   v1.18.0
```

## 16. 部署coredns

#### 16.1 安装依赖

```bash
yum -y install epel-release
yum -y install jq
```

#### 16.2 下载配置文件

```bash
mkdir -p ${SOFT_DIR}/coredns
cd ${SOFT_DIR}/coredns
wget https://raw.githubusercontent.com/coredns/deployment/master/kubernetes/coredns.yaml.sed
wget https://raw.githubusercontent.com/coredns/deployment/master/kubernetes/deploy.sh
```

#### 16.3 部署dns

```
chmod +x deploy.sh
./deploy.sh -i 10.96.0.10 > coredns.yml
kubectl apply -f coredns.yml
```

#### 16.4 测试集群DNS是否可用

```bash
kubectl run busybox --rm --image=busybox:1.28 -it
```

进入后执行nslookup kubernetes.default确认解析正常:

```bash
/ # nslookup kubernetes.default
Server:    10.96.0.10
Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

Name:      kubernetes.default
Address 1: 10.96.0.1 kubernetes.default.svc.cluster.local
```

## 17. 测试集群

```yaml
cat<< EOF >test.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deploy
  namespace: default
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp
      release: stabel
  template:
    metadata:
      labels:
        app: myapp
        release: stabel
        env: test
    spec:
      containers:
      - name: myapp
        image: wangyanglinux/myapp:v2
        imagePullPolicy: IfNotPresent
        ports:
        - name: http
          containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: myapp
  namespace: default
spec:
  type: ClusterIP
  selector:
    app: myapp
    release: stabel
  ports:
  - name: http
    port: 80
    targetPort: 80
EOF
kubectl apply -f test.yaml
```

查看相关信息

```bash
[root@m1 coredns]# kubectl get pod
NAME                           READY   STATUS              RESTARTS   AGE
myapp-deploy-c7b5fb585-2dxnq   0/1     ContainerCreating   0          14s
myapp-deploy-c7b5fb585-b2x49   0/1     ContainerCreating   0          14s
myapp-deploy-c7b5fb585-djdqw   0/1     ContainerCreating   0          14s
[root@m1 coredns]# kubectl get svc
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP   86m
myapp        ClusterIP   10.96.120.124   <none>        80/TCP    44s
[root@m1 ~]# curl 10.96.31.114
Hello MyApp | Version: v2 | <a href="hostname.html">Pod Name</a>
```

## 18. 添加node

#### 18.1 分发密钥

```
for n in n{1..3};do scp -r /root/.ssh $n:/root/;done
```

#### 18.2  安装docker及docker调优

#### 18.3 创建目录

```bash
for n in n{1..3};do ssh $n "mkdir -p /opt/cni/bin ${K8S_DIR}/{manifests,pki} /etc/cni/net.d/ /data/logs/kubernetes/{kubelet,kube-proxy}";done
```

#### 18.4 分发node所需软件

```bash
#kubernetes
cd kubernetes/server/bin/ 
for n in n{1..3};do scp kubelet kube-proxy root@$n:/usr/local/bin/;done
#CNI
cd - 
for n in n{1..3};do \
scp cni-plugins-linux-amd64-v0.8.5.tgz $n:/opt/cni/bin/;ssh root@$n "cd /opt/cni/bin && tar xf cni-plugins-linux-amd64-v0.8.5.tgz"; done
#flaned
cd -
for n in n{1..3};do scp flannel-v0.12.0-linux-amd64.tar.gz $n:/usr/local/bin/ && ssh $n "cd /usr/local/bin/ && tar xf flannel-v0.12.0-linux-amd64.tar.gz";done
```

#### 18.5 分发证书

```shell
for n in n{1..3};do scp ${K8S_SSL}/{flanneld.pem,flanneld-key.pem,ca.pem} root@$n:${K8S_SSL};done
```

#### 18.6 复制配置文件及启动文件到相应节点

```bash
for n in n{1..3};do scp -r /etc/cni $n:/etc/;done
for n in n{1..3};do scp ${K8S_DIR}/{kube-proxy.kubeconfig,kube-proxy.conf,bootstrap-kubelet.kubeconfig,kubelet-conf.yml} $n:${K8S_DIR};done
for n in n{1..3};do scp ${SYSTEM_SERVICE_DIR}/{kubelet.service,kube-proxy.service,flannel.service} $n:${SYSTEM_SERVICE_DIR};done
```

#### 18.7 启动服务

```bash
for n in n{1..3};do ssh $n "systemctl enable  flannel.service";done
for n in n{1..3};do ssh $n "systemctl enable  kube-proxy.service";done
for n in n{1..3};do ssh $n "systemctl enable  kubelet.service";done
```

查看node信息

## 19. 报错解决

1. Failed to list IPVS destinations, error: parseIP Error ip=[10 244 3 2 0 0 0 0 0 0 0 0 0 0 0 0]
   Failed to sync endpoint for service: 10.96.0.10:9153/TCP, err: parseIP Error ip=[10 244 3 2 0 0 0 0 0 0 0 0 0 0 0]

出现这种情况是由于CentOS 7.x 系统自带的 3.10.x 内核存在一些 Bugs，导致运行的 Docker、Kubernetes 不稳定 ,需要升级内核到4.4

```bash
rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm
yum --enablerepo=elrepo-kernel install -y kernel-lt
#设置开机从新内核启动
grub2-set-default 'CentOS Linux (4.4.218-1.el7.elrepo.x86_64) 7 (Core)'
#所有节点重启主机,加载内核
reboot
#查看内核
uname -r
4.4.218-1.el7.elrepo.x86_64
```

2. Not using --random-fully in the MASQUERADE rule for iptables because the local version of iptables does not support it

按照错误日志提示iptables 本地版本不支持升级iptables ,需要升级iptables(升级有风险)

```bash
#安装依赖
yum install -y gcc make libnftnl-devel libmnl-devel autoconf automake libtool bison flex  libnetfilter_conntrack-devel libnetfilter_queue-devel libpcap-devel
#编译安装iptables
export LC_ALL=C
wget https://www.netfilter.org/projects/iptables/files/iptables-1.6.2.tar.bz2
tar -xvf iptables-1.6.2.tar.bz2
cd iptables-1.6.2
./autogen.sh
./configure  
make && make install
#重启 kube-proxy 与 kubelet
for n in m{1..3} n{1..3};do ssh $n "systemctl restart kube-proxy.service kubelet.service";done
```

3.错误信息kube-proxy

```bash
error: error looking for path of conntrack: exec: "conntrack": executable file not found in $PATH
```

conntrack没有安装，是一个用户态的命令，用于控制内核中ip_conntrack模块的，该模块是用于处理链路追踪的工具,就是iptables和netfilter的关系。

```bash
for n in m{1..3} n{1..3};do ssh $n "yum install -y conntrack-tools";done
for n in m{1..3} n{1..3};do ssh $n "systemctl restart kube-proxy.service kubelet.service";done
```

## 20.参数优化

1.主机宕机，运行在上面的POD不调度或者调度时间太久？

在kube-controller-manager加入以下配置

```bash
--node-monitor-grace-period=10s \
--node-monitor-period=3s \
--node-startup-grace-period=20s \
--pod-eviction-timeout=10s \
```

解释：
kubernetes节点失效后pod的调度过程：

- Master每隔一段时间和node联系一次，判定node是否失联，这个时间周期配置项为 node-monitor-period ，默认5s
- 当node失联后一段时间后，kubernetes判定node为notready状态，这段时长的配置项为 node-monitor-grace-period ，默认40s
- 当node失联后一段时间后，kubernetes判定node为unhealthy，这段时长的配置项为 node-startup-grace-period ，默认1m0s
- 当node失联后一段时间后，kubernetes开始删除原node上的pod，这段时长配置项为 pod-eviction-timeout ，默认5m0s

在应用中，想要缩短pod的重启时间，可以修改上述几个[参数](https://kubernetes.io/zh/docs/reference/command-line-tools-reference/kube-controller-manager/)

2.kubelet镜像拉取策略

修改kubelet配置文件

```bash
# 逐一拉取镜像 默认值为 true
--serialize-image-pulls=false
# 如果在该参数值所设置的期限之前没有拉取镜像的进展，镜像拉取操作将被取消。默认值为 1m0s
--image-pull-progress-deadline=30s
```

3.kubelet重启无法获得容器运行时的正确状态

修改kubelet配置文件

```bash
--node-status-update-frequency=30s
```

4.etcd频繁选举leader导致集群不可用
还有类似心跳检测超时的情况

解决方式：

1、集群中有某些机器时间不同步
2、扩大心跳检测时长

修改etcd.service配置

```bash
--election-timeout=5000 \
--heartbeat-interval=500 \
```

