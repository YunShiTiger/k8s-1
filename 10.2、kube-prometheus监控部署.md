## 相关地址信息

Prometheus github 地址：https://github.com/coreos/kube-prometheus

## 组件说明

- MetricServer：是kubernetes集群资源使用情况的聚合器，收集数据给kubernetes集群内使用，如kubectl,hpa,scheduler等。
- PrometheusOperator：是一个系统监测和警报工具箱，用来存储监控数据。
- NodeExporter：用于各node的关键度量指标状态数据。
- KubeStateMetrics：收集kubernetes集群内资源对象数据，制定告警规则。
- Prometheus：采用pull方式收集apiserver，scheduler，controller-manager，kubelet组件数据，通过http协议传输。
- Grafana：是可视化数据统计和监控平台。

## 下载kube-prometheus

下载kube-prometheus

```bash
git clone https://github.com/coreos/kube-prometheus.git    
cd kube-prometheus/manifests
```

安装CRD

```bash
kubectl create -f setup/
```

## kube-prometheus数据持久化

添加持久化 prometheus-prometheus.yaml

```yaml
...
spec:
  retention: 90d #数据持久化时间
  storage:
    volumeClaimTemplate:
      spec:
        storageClassName: prometheus-data-db
        resources:
          requests:
            storage: 10Gi
...
```

声明StorageClass 对象

```yaml
cat << 'EOF' >prometheus-storageclass.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: prometheus-data-db
provisioner: fuseim.pri/ifs
EOF
```

## grafana数据持久化

创建pvc

```yaml
cat << 'EOF' >grafana-volume.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: grafana-set
  namespace: monitoring
spec:
  storageClassName: prometheus-data-db #---指定StorageClass
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
EOF
```

修改grafana-deployment.yaml

```yaml
#      - emptyDir: {}
#        name: grafana-storage
#添加
      - name: grafana-storage
        persistentVolumeClaim:
          claimName: grafana-set
```

部署kube-prometheus

```bash
kubectl apply -f ./
```

查看pod

```bash
kubectl get pod -n monitoring
```

查看cpu

```bash
kubectl top node
```

## ingress暴露服务

```yaml
cat << 'EOF' >prometheus-ingress.yaml 
apiVersion: traefik.containo.us/v1alpha1
kind: IngressRoute
metadata:
  name: grafana
  namespace: monitoring
spec:
  entryPoints:
    - web
  routes:
  - match: Host(`grafana.wzxmt.com`) && PathPrefix(`/`)
    kind: Rule
    services:
    - name: grafana
      port: 3000
---
apiVersion: traefik.containo.us/v1alpha1
kind: IngressRoute
metadata:
  name: prometheus
  namespace: monitoring
spec:
  entryPoints:
    - web
  routes:
  - match: Host(`prometheus.wzxmt.com`) && PathPrefix(`/`)
    kind: Rule
    services:
    - name: prometheus-k8s
      port: 9090
---
apiVersion: traefik.containo.us/v1alpha1
kind: IngressRoute
metadata:
  name: alertmanager
  namespace: monitoring
spec:
  entryPoints:
    - web
  routes:
  - match: Host(`alertmanager.wzxmt.com`) && PathPrefix(`/`)
    kind: Rule
    services:
    - name: alertmanager-main
      port: 9093
EOF
kubectl apply -f prometheus-ingress.yaml 
```

查看IngressRoute

```bash
kubectl get ingressroute -n monitoring
```

## 访问prometheus组件

http://prometheus.wzxmt.com
http://alertmanager.wzxmt.com
http://grafana.wzxmt.com

## kube-controller-manager 监控

```yaml
cat << 'EOF' >prometheus-kubeControllerManagerService.yaml
apiVersion: v1
kind: Service
metadata:
  namespace: kube-system
  name: kube-controller-manager
  labels:
    k8s-app: kube-controller-manager
spec:
  type: ClusterIP
  ports:
  - name: http-metrics
    port: 10252
    targetPort: 10252
    protocol: TCP
---
apiVersion: v1
kind: Endpoints
metadata:
  labels:
    k8s-app: kube-controller-manager
  name: kube-controller-manager
  namespace: kube-system
subsets:
- addresses:
  - ip: 10.0.0.31
  - ip: 10.0.0.32
  - ip: 10.0.0.33
  ports:
  - name: http-metrics
    port: 10252
    protocol: TCP
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    k8s-app: kube-controller-manager
  name: kube-controller-manager
  namespace: monitoring
spec:
  endpoints:
  - interval: 30s
    metricRelabelings:
    - action: drop
      regex: etcd_(debugging|disk|request|server).*
      sourceLabels:
      - __name__
    port: http-metrics
  jobLabel: k8s-app
  namespaceSelector:
    matchNames:
    - kube-system
  selector:
    matchLabels:
      k8s-app: kube-controller-manager
EOF
kubectl apply -f prometheus-kubeControllerManagerService.yaml
```

## kube-scheduler 监控

```yaml
cat << 'EOF' >prometheus-kubeSchedulerService.yaml
apiVersion: v1
kind: Service
metadata:
  namespace: kube-system
  name: kube-scheduler
  labels:
    k8s-app: kube-scheduler
spec:
  type: ClusterIP
  ports:
  - name: port
    port: 10251
    protocol: TCP
---
apiVersion: v1
kind: Endpoints
metadata:
  labels:
    k8s-app: kube-scheduler
  name: kube-scheduler
  namespace: kube-system
subsets:
- addresses:
  - ip: 10.0.0.31
  - ip: 10.0.0.32
  - ip: 10.0.0.33
  ports:
  - name: http-metrics
    port: 10251
    protocol: TCP
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    k8s-app: kube-scheduler
  name: kube-scheduler
  namespace: monitoring
spec:
  endpoints:
  - interval: 30s  # 每30s获取一次信息
    port: http-metrics  # 对应 service 的端口名
  jobLabel: k8s-app
  namespaceSelector:  # 表示去匹配某一命名空间中的service，如果想从所有的namespace中匹配用any: true
    matchNames:
    - kube-system
  selector:  # 匹配的 Service 的 labels，如果使用 mathLabels，则下面的所有标签都匹配时才会匹配该 service，如果使用 matchExpressions，则至少匹配一个标签的 service 都会被选择
    matchLabels:
      k8s-app: kube-scheduler
EOF
kubectl apply -f prometheus-kubeSchedulerService.yaml
```

## coredns 监控

```yaml
cat << 'EOF' >serviceMonitorCoreDNS.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    k8s-app: kube-dns
  name: coredns
  namespace: monitoring
spec:
  endpoints:
  - interval: 30s
    port: metrics
  jobLabel: k8s-app
  namespaceSelector:
    matchNames:
    - kube-system
  selector:
    matchLabels:
      k8s-app: kube-dns
EOF
kubectl apply -f serviceMonitorCoreDNS.yaml
```

## 自定义 Kube-Prometheus 监控项

前面我们讲解了 如何快速部署 Kube-Prometheus 监控系统，下面我们继续介绍如何在 Kube-Prometheus 中添加一个自定义的监控项。除了 Kubernetes 集群中的一些资源对象、节点以及组件需要监控，有的时候我们可能还需要根据实际的业务需求去添加自定义的监控项，添加一个自定义监控的步骤也是非常简单的。

- **第一步：建立一个 ServiceMonitor 对象，用于 Prometheus 添加监控项；**
- **第二步：为 ServiceMonitor 对象关联 metrics 数据接口的一个 Service 对象；**
- **第三步：确保 Service 对象可以正确获取到 metrics 数据。**

接下来我们就来看看如何添加 Etcd 集群的监控。无论是 Kubernetes 集群外的还是使用 Kubeadm 安装在集群内部的 Etcd 集群，我们这里都将其视作集群外的独立集群，因为对于二者的使用方法没什么特殊之处。

## 添加EDCT监控项

添加secret

```bash
cd /etc/kubernetes/pki/
kubectl create secret generic etcd-ssl --from-file=ca.pem \
--from-file=etcd.pem --from-file=etcd-key.pem -n monitoring
```

修改 prometheus-prometheus.yaml 资源文件，并添加内容如下：

```
apiVersion: monitoring.coreos.com/v1
kind: Prometheus
......
  replicas: 2
  secrets:
  - etcd-ssl#  添加secret名称
......
```

更新配置，查看对象

```
# 更新资源文件
kubectl apply -f prometheus-prometheus.yaml
# 查看pod状态
kubectl get pod -n monitoring
# 查看证书
kubectl exec -it -n monitoring prometheus-k8s-0 ls /etc/prometheus/secrets/etcd-ssl/
```

创建ServiceMonitor

```yaml
cat << 'EOF' >prometheus-serviceMonitorEtcd.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: etcd
  namespace: monitoring
  labels:
    k8s-app: etcd-k8s
spec:
  jobLabel: k8s-app
  endpoints:
  - port: port
    interval: 30s
    scheme: https
    tlsConfig:
      caFile: /etc/prometheus/secrets/etcd-ssl/ca.pem
      certFile: /etc/prometheus/secrets/etcd-ssl/etcd.pem
      keyFile: /etc/prometheus/secrets/etcd-ssl/etcd-key.pem
      insecureSkipVerify: true
  selector:
    matchLabels:
      k8s-app: etcd
  namespaceSelector:
    matchNames:
    - kube-system
EOF
kubectl apply -f prometheus-serviceMonitorEtcd.yaml
```

定义ETCD Service 对象

```yaml
cat << 'EOF' >prometheus-EtcdServiceEnpoints.yaml
apiVersion: v1
kind: Service
metadata:
  name: etcd
  namespace: kube-system
  labels:
    k8s-app: etcd
spec:
  type: ClusterIP
  clusterIP: None
  ports:
  - name: port
    port: 2379
    protocol: TCP
---
apiVersion: v1
kind: Endpoints
metadata:
  name: etcd
  namespace: kube-system
  labels:
    k8s-app: etcd
subsets:
- addresses:
  - ip: 10.0.0.31
    nodeName: etcd01
  - ip: 10.0.0.32
    nodeName: etcd02
  - ip: 10.0.0.33
    nodeName: etcd03
  ports:
  - name: port
    port: 2379
    protocol: TCP
EOF
kubectl apply -f prometheus-EtcdServiceEnpoints.yaml
```

创建完成后，稍等一会我们可以去Prometheus 里面查看targets，便会出现etcd监控信息：

![image-20200606140621807](acess/image-20200606140621807.png)

## 添加Traefik 监控

查看Traefik Service标签

```bash
kubectl get svc --show-labels -n ingress-system

NAME      TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                            AGE   LABELS
traefik   ClusterIP   None         <none>        80/TCP,8080/TCP,443/TCP,8082/TCP   43h   k8s-app=traefik
```

修改标签

```bash
kubectl label svc traefik  -n ingress-system k8s-app=traefik-ingress --overwrite
```

配置服务监控资源

```yaml
cat << 'EOF' >prometheus-serviceMonitorTraefik.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: traefik-ingress
  namespace: monitoring
  labels:
    k8s-app: traefik-ingress
spec:
  jobLabel: k8s-app
  endpoints:
  - port: admin              #---设置为metrics地址所对应的ports名称
    interval: 30s
  selector:
    matchLabels:
      k8s-app: traefik-ingress
  namespaceSelector:
    matchNames:
    - ingress-system
EOF
kubectl apply -f prometheus-serviceMonitorTraefik.yaml
```

## 服务自动发现

如果在我们的 Kubernetes 集群中有了很多的Service/Pod，那么我们都需要一个一个的去建立一个对应的 ServiceMonitor 对象来进行监控吗？这样岂不是又变得麻烦起来了？

为解决上面的问题，Prometheus Operator 为我们提供了一个额外的抓取配置的来解决这个问题，我们可以通过添加额外的配置来进行服务发现进行自动监控。和前面自定义的方式一样，我们想要在 Prometheus Operator 当中去自动发现并监控具有prometheus.io/scrape=true这个 annotations 的 Service

```yaml
cat<< 'EOF' >prometheus-additional.yaml
- job_name: 'kubernetes-service-endpoints'
  kubernetes_sd_configs:
  - role: endpoints
  relabel_configs:
  - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
    action: keep
    regex: true
  - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
    action: replace
    target_label: __scheme__
    regex: (https?)
  - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
    action: replace
    target_label: __metrics_path__
    regex: (.+)
  - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
    action: replace
    target_label: __address__
    regex: ([^:]+)(?::\d+)?;(\d+)
    replacement: $1:$2
  - action: labelmap
    regex: __meta_kubernetes_service_label_(.+)
  - source_labels: [__meta_kubernetes_namespace]
    action: replace
    target_label: kubernetes_namespace
  - source_labels: [__meta_kubernetes_service_name]
    action: replace
    target_label: kubernetes_name
EOF
```

要想实现自动发现集群中的Service，需要我们在 Service 的`annotation`区域添加`prometheus.io/scrape=true`声明

创建Secret 对象

```bash
kubectl create secret generic additional-configs --from-file=prometheus-additional.yaml -n monitoring
```

添加配置：

```yaml
# prometheus-prometheus.yaml添加以下内容

    serviceAccountName: prometheus-k8s
    additionalScrapeConfigs:
      name: additional-configs
      key: prometheus-additional.yaml
```

更新 prometheus 资源对象：

```bash
kubectl apply -f prometheus-prometheus.yaml
```

隔一小会儿，可以前往 Prometheus 的 Dashboard 中查看配置是否生效：

在 Prometheus Dashboard 的配置页面下面我们可以看到已经有了对应的的配置信息了，但是我们切换到 targets 页面下面却并没有发现对应的监控任务，查看 Prometheus 的 Pod 日志：

```bash
# kubectl logs -f prometheus-k8s-0 prometheus -n monitoring

level=error ts=2020-05-25T14:45:30.780Z caller=klog.go:94 component=k8s_client_runtime func=ErrorDepth msg="/app/discovery/kubernetes/kubernetes.go:262: Failed to list *v1.Service: services is forbidden: User \"system:serviceaccount:monitoring:prometheus-k8s\" cannot list resource \"services\" in API group \"\" at the cluster scope"
```

可以看到有很多错误日志出现，都是`xxx is forbidden`，这说明是 RBAC 权限的问题，通过 prometheus 资源对象的配置可以知道 Prometheus 绑定了一个名为 prometheus-k8s 的 ServiceAccount 对象，而这个对象绑定的是一个名为 prometheus-k8s 的 ClusterRole：

```yaml
#cat prometheus-clusterRole.yaml

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prometheus-k8s
rules:
- apiGroups:
  - ""
  resources:
  - nodes/metrics
  verbs:
  - get
- nonResourceURLs:
  - /metrics
  verbs:
  - get
```

上面的权限规则中我们可以看到明显没有对Service或者Pod的list权限，添加权限：

```yaml
cat << EOF >prometheus-clusterRole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prometheus-k8s
rules:
- apiGroups:
  - ""
  resources:
  - nodes
  - services
  - endpoints
  - pods
  - nodes/proxy
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - configmaps
  - nodes/metrics
  verbs:
  - get
- nonResourceURLs:
  - /metrics
  verbs:
  - get
EOF
kubectl apply -f  prometheus-clusterRole.yaml
```

如果5分钟不出现重建下 Prometheus 的所有 Pod，正常就可以看到 targets 页面下面有 kubernetes-service-endpoints 这个监控任务了

例：

```yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: grafana
  name: grafana
  namespace: monitoring
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "3000"
spec:
  ports:
  - name: http
    port: 3000
    targetPort: http
  selector:
    app: grafana
```

查看kubernetes-service-endpoints下是否有grafana

![image-20200525225603536](acess/image-20200525225603536.png)

后期其他应用，只需在service添加这两行

```yaml
annotations:
  prometheus.io/scrape: "true"
  prometheus.io/port: "9121"
```

## 部署blackbox-exporter

创建目录

```bash
mkdir -p blackbox-exporter
cd blackbox-exporter
```

私有镜像

```bash
docker pull quay.io/prometheus/blackbox-exporter:v0.18.0
docker tag quay.io/prometheus/blackbox-exporter:v0.18.0 harbor.wzxmt.com/k8s/blackbox-exporter:v0.18.0
docker push harbor.wzxmt.com/k8s/blackbox-exporter:v0.18.0
```

ConfigMap

```yaml
cat << 'EOF' >cm.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    apps: blackbox-exporter
  name: blackbox-exporter
  namespace: monitoring
data:
  blackbox.yml: |-
    "modules":
      "http_2xx":
        "http":
          "preferred_ip_protocol": "ip4"
        "prober": "http"
      "http_post_2xx":
        "http":
          "method": "POST"
          "preferred_ip_protocol": "ip4"
        "prober": "http"
      "irc_banner":
        "prober": "tcp"
        "tcp":
          "preferred_ip_protocol": "ip4"
          "query_response":
          - "send": "NICK prober"
          - "send": "USER prober prober prober :prober"
          - "expect": "PING :([^ ]+)"
            "send": "PONG ${1}"
          - "expect": "^:[^ ]+ 001"
      "pop3s_banner":
        "prober": "tcp"
        "tcp":
          "preferred_ip_protocol": "ip4"
          "query_response":
          - "expect": "^+OK"
          "tls": true
          "tls_config":
            "insecure_skip_verify": false
      "ssh_banner":
        "prober": "tcp"
        "tcp":
          "preferred_ip_protocol": "ip4"
          "query_response":
          - "expect": "^SSH-2.0-"
      "tcp_connect":
        "prober": "tcp"
        "tcp":
          "preferred_ip_protocol": "ip4"
EOF
```

Deployment

```yaml
cat << 'EOF' >dp.yaml
kind: Deployment
apiVersion: apps/v1
metadata:
  name: blackbox-exporter
  namespace: monitoring
  labels:
    apps: blackbox-exporter
spec:
  replicas: 1
  selector:
    matchLabels:
      apps: blackbox-exporter
  template:
    metadata:
      labels:
        apps: blackbox-exporter
    spec:
      volumes:
      - name: config
        configMap:
          name: blackbox-exporter
          defaultMode: 420
      containers:
      - name: blackbox-exporter
        image: harbor.wzxmt.com/k8s/blackbox-exporter:v0.18.0
        args:
        - --config.file=/etc/blackbox_exporter/blackbox.yml
        - --log.level=debug
        - --web.listen-address=:9115
        ports:
        - name: blackbox-port
          containerPort: 9115
          protocol: TCP
        resources:
          limits:
            cpu: 200m
            memory: 256Mi
          requests:
            cpu: 100m
            memory: 50Mi
        volumeMounts:
        - name: config
          mountPath: /etc/blackbox_exporter
        readinessProbe:
          tcpSocket:
            port: 9115
          initialDelaySeconds: 5
          timeoutSeconds: 5
          periodSeconds: 10
          successThreshold: 1
          failureThreshold: 3
        imagePullPolicy: IfNotPresent
      imagePullSecrets:
      - name: harborlogin
      restartPolicy: Always
EOF
```

Service

```yaml
cat << 'EOF' >svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: blackbox-exporter
  namespace: monitoring
  labels:
    apps: blackbox-exporter
spec:  
  type: ClusterIP  
  selector:    
    apps: blackbox-exporter 
  ports:  
  - name: blackbox-exporter    
    port: 9115    
    targetPort: 9115
EOF
```

Ingress

```yaml
cat << 'EOF' >ingress.yaml
apiVersion: traefik.containo.us/v1alpha1
kind: IngressRoute
metadata:
  name: blackbox-exporter
  namespace: monitoring
  labels:
    apps: blackbox-exporter
spec:
  entryPoints:
    - web
  routes:
  - match: Host(`blackbox.wzxmt.com`) && PathPrefix(`/`)
    kind: Rule
    services:
    - name: blackbox-exporter
      port: 9115
EOF
```

serviceMonitor

```yaml
cat << 'EOF' >serviceMonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    apps: blackbox-exporter
  name: blackbox-exporter
  namespace: monitoring
spec:
  endpoints:
  - interval: 30s
    port: blackbox-exporter
  jobLabel: app
  namespaceSelector:
    matchNames:
    - monitoring
  selector:
    matchLabels:
      apps: blackbox-exporter
EOF
```

创建docker-registry

```bash
kubectl create secret docker-registry harborlogin \
--namespace=monitoring  \
--docker-server=https://harbor.wzxmt.com \
--docker-username=admin \
--docker-password=admin
```

部署

```bash
kubectl apply -f ./
```

浏览器访问

http://blackbox.wzxmt.com

**注意：在pod控制器上加annotations，并重启pod，监控生效**
配置范例：

```bash
"annotations": {
  "blackbox_port": "20880",
  "blackbox_scheme": "tcp"
}
```

- blackbox_http_pod_probe

  > 监控http协议服务是否存活

```
"annotations": {
  "blackbox_path": "/",
  "blackbox_port": "8080",
  "blackbox_scheme": "http"
}
```

## 部署cadvisor

修改运算节点软连接所有运算节点上：

```bash
mount -o remount,rw /sys/fs/cgroup/
ln -s /sys/fs/cgroup/cpu,cpuacct/ /sys/fs/cgroup/cpuacct,cpu
ll /sys/fs/cgroup|grep cpu
```

```yaml
cat << 'EOF' >ds.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: cadvisor
  namespace: monitoring
  labels:
    app: cadvisor
spec:
  selector:
    matchLabels:
      name: cadvisor
  template:
    metadata:
      labels:
        name: cadvisor
    spec:
      hostNetwork: true
      tolerations:
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      containers:
      - name: cadvisor
        image: google/cadvisor:v0.33.0
        imagePullPolicy: IfNotPresent
        volumeMounts:
        - name: rootfs
          mountPath: /rootfs
          readOnly: true
        - name: var-run
          mountPath: /var/run
          readOnly: false
        - name: sys
          mountPath: /sys
          readOnly: true
        - name: docker
          mountPath: /var/lib/docker
          readOnly: true
        ports:
          - name: http
            containerPort: 4194
            protocol: TCP
        readinessProbe:
          tcpSocket:
            port: 4194
          initialDelaySeconds: 5
          periodSeconds: 10
        args:
          - --housekeeping_interval=10s
          - --port=4194
      imagePullSecrets:
      - name: harborlogin
      terminationGracePeriodSeconds: 30
      volumes:
      - name: rootfs
        hostPath:
          path: /
      - name: var-run
        hostPath:
          path: /var/run
      - name: sys
        hostPath:
          path: /sys
      - name: docker
        hostPath:
          path: /data/docker
EOF
kubectl apply -f ds.yaml
```

**注意：在pod控制器上加annotations，并重启pod，监控生效**

```
"annotations": {
  "prometheus_io_scrape": "true",
  "prometheus_io_port": "12346",
  "prometheus_io_path": "/"
}
```

## grafana插件

- Kubernetes App

```
grafana-cli plugins install grafana-kubernetes-app
```

- Clock Pannel

```
grafana-cli plugins install grafana-clock-panel
```

- Pie Chart

```
grafana-cli plugins install grafana-piechart-panel
```

- D3 Gauge

```
grafana-cli plugins install briangann-gauge-panel
```

- Discrete

```
grafana-cli plugins install natel-discrete-panel
```

仪表盘： Get this dashboard:

```
8588
10566
11600
10856
11543    blackbox
7587
```

